{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn import ensemble, linear_model, metrics\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "from tqdm import *\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "\n",
    "def r_score(y_true, y_pred, sample_weight=None, multioutput=None):\n",
    "    r2 = r2_score(y_true, y_pred, sample_weight=sample_weight,\n",
    "                  multioutput=multioutput)\n",
    "    r = (np.sign(r2) * np.sqrt(np.abs(r2)))\n",
    "    if r <= -1:\n",
    "        return -1\n",
    "    else:\n",
    "        return r\n",
    "\n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "\n",
    "# import kagglegym\n",
    "# o = env.reset()\n",
    "# env = kagglegym.make()\n",
    "# train = o.train\n",
    "\n",
    "train = pd.read_hdf('../input/train.h5')\n",
    "\n",
    "low_y_cut = -0.086093\n",
    "high_y_cut = 0.093497\n",
    "\n",
    "y_is_above_cut = (train.y > high_y_cut)\n",
    "y_is_below_cut = (train.y < low_y_cut)\n",
    "y_is_within_cut = (~y_is_above_cut & ~y_is_below_cut)\n",
    "train = train.loc[y_is_within_cut, :]\n",
    "\n",
    "d_mean = train.median(axis=0)\n",
    "train[\"nbnulls\"] = train.isnull().sum(axis=1)\n",
    "\n",
    "rnd = 17\n",
    "\n",
    "# keeping na information on some columns (best selected by the tree algorithms)\n",
    "add_nas_ft = False\n",
    "nas_cols = ['technical_9', 'technical_0', 'technical_32', 'technical_16', 'technical_38',\n",
    "            'technical_44', 'technical_20', 'technical_30', 'technical_13']\n",
    "# columns kept for evolution from one month to another (best selected by the tree algorithms)\n",
    "add_diff_ft = True\n",
    "diff_cols = ['technical_22', 'technical_20', 'technical_30', 'technical_13', 'technical_34']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if add_diff_ft:\n",
    "    train = train.sort_values(by=['id', 'timestamp'])\n",
    "    for elt in diff_cols:\n",
    "        # a quick way to obtain deltas from one month to another but it is false on the first\n",
    "        # month of each id\n",
    "        train[elt + \"_d\"] = train[elt].rolling(2).apply(lambda x: x[1] - x[0]).fillna(0)\n",
    "        gc.collect()\n",
    "    # removing month 0 to reduce the impact of erroneous deltas\n",
    "    train = train[train.timestamp != 0]\n",
    "\n",
    "cols = [x for x in train.columns if x not in ['id', 'timestamp', 'y', 'sample']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbose = 1\n",
    "plot = 1\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'regression',\n",
    "    'verbose': verbose,\n",
    "    # 'num_leaves': 1024,\n",
    "    # 'learning_rate': 0.05,\n",
    "    # 'feature_fraction': 0.9,\n",
    "    # 'bagging_fraction': 0.8,\n",
    "    # 'bagging_freq': 5,\n",
    "    # 'max_bin': 512,\n",
    "    # 'num_iterations':20\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train[cols], train.y, test_size=0.2, random_state=0)\n",
    "train_data = lgb.Dataset(X_train, y_train)\n",
    "gc.collect()\n",
    "bst = lgb.train(params, train_data, feval=r_score, verbose_eval=verbose)\n",
    "\n",
    "df_fi = pd.DataFrame(bst.feature_name(), columns=['feature'])\n",
    "df_fi['importance'] = list(bst.feature_importance('gain'))\n",
    "df_fi.sort_values('importance', ascending=False, inplace=True)\n",
    "print_full(df_fi)\n",
    "if plot:\n",
    "    plt.figure()\n",
    "    df_fi.head(10).plot(kind='barh',\n",
    "                        x='feature',\n",
    "                        y='importance',\n",
    "                        sort_columns=False,\n",
    "                        legend=False,\n",
    "                        figsize=(10, 6),\n",
    "                        facecolor='#1DE9B6',\n",
    "                        edgecolor='white')\n",
    "\n",
    "    plt.title('LightGBM Feature Importance')\n",
    "    plt.xlabel('relative importance')\n",
    "    plt.show()\n",
    "\n",
    "if verbose:\n",
    "    y_pred = bst.predict(X_test)\n",
    "    print('\\nScore for another fold: ', r_score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1687552, 117)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.id.nunique()\n",
    "train.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
